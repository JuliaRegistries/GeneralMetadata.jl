using TOML: TOML
using DataStructures: DefaultDict, DefaultOrderedDict, OrderedDict
using CSV: CSV
using DataFrames: DataFrames, DataFrame, groupby, combine, transform, combine, eachrow

function main()
    sql = """
        COPY (SELECT
            (elem.value ->> 0)::integer AS link_type,
            l.url,
            p.repo,
            p.effname,
            p.rawversion,
            p.arch,
            p.origversion,
            p.versionclass
        FROM packages p
        CROSS JOIN LATERAL json_array_elements(p.links) AS elem(value)
        JOIN links l ON l.id = (elem.value ->> 1)::integer
        WHERE (NOT p.shadow='t') AND
              ((elem.value ->> 0)::integer = 1 or (elem.value ->> 0)::integer = 2))
        TO '/tmp/out.csv' WITH (format csv);
    """
    run(`psql -U repology -c $sql`)
    df = CSV.read("/tmp/out.csv", DataFrame, header=["link_type",
           "url",
           "repo",
           "effname",
           "arch",
           "rawversion",
           "origversion",
           "versionclass"])
    # We'll only trust download URLs when only one was used for the given package
    # Many packages have more than one download, but then we don't know if that was a (build) dependency
    # or the actual project or perhaps some larger meta-project that vendors lots of deps.
    filtered_df = df |>
        x -> groupby(x, [:repo, :effname, :rawversion, :arch]) |>
        x -> transform(x, :url => (x -> length(unique(x))) => :n_unique_urls) |>
        x -> filter(row -> row.n_unique_urls == 1, x) |>
        # And also remove any rows for which the same URL points at two different effnames:
        x -> groupby(x, :url) |>
        x -> transform(x, :effname => (x -> length(unique(x))) => :n_unique_effnames) |>
        x -> filter(row -> row.n_unique_effnames == 1, x)

    # For repositories (link_type == 2), ignore versions:
    repositories = DefaultDict{String,Vector{String}}(()->String[])
    for row in eachrow(filtered_df[filtered_df.link_type .== 2, :])
        push!(repositories[row.effname], row.url)
    end

    # For downloads (link_type == 1), the versions may be meaningfully linked to the URL
    url_groups = combine(groupby(filtered_df[filtered_df.link_type .== 1, :], [:url, :effname]), :origversion => (x -> [unique(x)]) => :versions)
    download_patterns = DefaultDict{String, Vector{String}}(()->String[])
    for row in eachrow(url_groups)
        length(row.versions) == 1 || continue
        # Ignore download URLs that don't explicitly include the URL
        # these are often things like -latest or the like
        v = only(row.versions)
        contains(row.url, v) || continue
        contains("\\E(.+)\\Q", v) && continue # These are invalid versions and corrupt the Regex
        rs = download_patterns[row.effname]
        any(r->!isnothing(match(Regex(r), row.url)), rs) && continue # We already have this pattern
        pattern = string("^\\Q", row.url, "\\E\$")
        pattern = replace(pattern, v => "\\E(.+)\\Q"; count = 1)
        pattern = replace(pattern, v => "\\E\\1\\Q")
        push!(rs, pattern)
    end

    sql = """
        COPY (SELECT
            p.effname,
            p.cpe_vendor || ':' || p.cpe_product AS cpe
        FROM project_cpe p
        WHERE p.cpe_vendor IS NOT NULL AND p.cpe_product IS NOT NULL)
        TO '/tmp/out.csv' WITH (format csv);
    """
    run(`psql -U repology -c $sql`)
    cpes = CSV.read("/tmp/out.csv", DataFrame, header=["effname", "cpe"])

    repology_info = Dict{String, Any}()
    for project in union(keys(download_patterns), keys(repositories))
        d = Dict()
        repos = repositories[project]
        !isempty(repos)     && (d["repositories"] = repos)
        patterns = download_patterns[project]
        !isempty(patterns)  && (d["url_patterns"] = patterns)
        proj_cpes = cpes[cpes.effname .== project, :cpe]
        !isempty(proj_cpes) && (d["cpes"] = proj_cpes)
        repology_info[project] = d
    end

    open(joinpath(@__DIR__, "..", "repology_info.toml"), "w") do f
        println(f, "# This file is autogenerated with data from https://repology.com")
        # TODO: store the actual date of the dump, somehow
        TOML.print(f, repology_info, sorted = true)
    end

    return repology_info
end

if abspath(PROGRAM_FILE) == @__FILE__
    main()
end
